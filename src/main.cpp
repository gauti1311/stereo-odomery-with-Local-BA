#include "opencv2/video/tracking.hpp"
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/features2d/features2d.hpp"
#include "opencv2/calib3d/calib3d.hpp"

#include <iostream>
#include <ctype.h>
#include <algorithm>
#include <iterator>
#include <vector>
#include <ctime>
#include <sstream>
#include <fstream>
#include <string>
#include <array>

#include "feature.h"
#include "utils.h"
#include "evaluate_odometry.h"
#include "visualOdometry.h"
#include "Frame.h"

#include "camera_object.h"
#include "rgbd_standalone.h"
#include "optimizer.h"

using namespace std;

int main(int argc, char **argv)
{

    #if USE_CUDA
        printf("CUDA Enabled\n");
    #endif
    // -----------------------------------------
    // Load images and calibration parameters
    // -----------------------------------------
    bool display_ground_truth = false;
    bool use_intel_rgbd = false;
    std::vector<Matrix> pose_matrix_gt;
    int windowSize = 8;  

    std::deque<std::array<double,6>> poses(windowSize);
    std::deque<cv::Mat> points3D(windowSize);
    std::deque<std::vector<cv::Point2f>> points2D(windowSize);

    if(argc == 4)
    {   display_ground_truth = true;
        cerr << "Display ground truth trajectory" << endl;
        // load ground truth pose
        string filename_pose = string(argv[3]);
        pose_matrix_gt = loadPoses(filename_pose);

    }
    if(argc < 3)
    {
        cerr << "Usage: ./run path_to_sequence(rgbd for using intel rgbd) path_to_calibration [optional]path_to_ground_truth_pose" << endl;
        return 1;
    }

    // Sequence
    string filepath = string(argv[1]);
    cout << "Filepath: " << filepath << endl;

    if(filepath == "rgbd") use_intel_rgbd = true;

    // Camera calibration
    string strSettingPath = string(argv[2]);
    cout << "Calibration Filepath: " << strSettingPath << endl;

    cv::FileStorage fSettings(strSettingPath, cv::FileStorage::READ);

    
    float fx = fSettings["Camera.fx"];
    float fy = fSettings["Camera.fy"];
    float cx = fSettings["Camera.cx"];
    float cy = fSettings["Camera.cy"];
    float bf = fSettings["Camera.bf"];

    double scale; 

    cv::Mat projMatrl = (cv::Mat_<float>(3, 4) << fx, 0., cx, 0., 0., fy, cy, 0., 0,  0., 1., 0.);
    cv::Mat projMatrr = (cv::Mat_<float>(3, 4) << fx, 0., cx, bf, 0., fy, cy, 0., 0,  0., 1., 0.);
    cv::Mat intrinsic = (cv::Mat_<double>(3, 3) << fx, 0., cx, 0., fy, cy, 0,  0., 1.);
    
    cout << "P_left: " << endl << projMatrl << endl;
    cout << "P_right: " << endl << projMatrr << endl;

    // -----------------------------------------
    // Initialize variables
    // -----------------------------------------
    cv::Mat rotation = cv::Mat::eye(3, 3, CV_64F);
    cv::Mat translation = cv::Mat::zeros(3, 1, CV_64F);
    cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);
    cv::Mat rvec1 = cv::Mat::zeros(3, 1, CV_64FC1);

    cv::Mat pose = cv::Mat::zeros(3, 1, CV_64F);
    cv::Mat Rpose = cv::Mat::eye(3, 3, CV_64F);
    std::array<double,6> camPose;
    
    cv::Mat frame_pose = cv::Mat::eye(4, 4, CV_64F);
    cv::Mat frame_pose32 = cv::Mat::eye(4, 4, CV_32F);

    std::cout << "frame_pose " << frame_pose << std::endl;
    cv::Mat trajectory = cv::Mat::zeros(600, 1200, CV_8UC3);
    FeatureSet currentVOFeatures;
    cv::Mat points4D;
    int init_frame_id = 0;

    // ------------------------
    // Load first images
    // ------------------------
    cv::Mat imageRight_t0,  imageLeft_t0;
    CameraBase *pCamera = NULL;
    if(use_intel_rgbd)
    {   
        pCamera = new Intel_V4L2;
        for (int throw_frames = 10 ; throw_frames >=0 ; throw_frames--)
            pCamera->getLRFrames(imageLeft_t0,imageRight_t0);
    }
    else
    {
        cv::Mat imageLeft_t0_color;
        loadImageLeft(imageLeft_t0_color,  imageLeft_t0, init_frame_id, filepath);
        
        cv::Mat imageRight_t0_color;  
        loadImageRight(imageRight_t0_color, imageRight_t0, init_frame_id, filepath);
    }
    clock_t t_a, t_b;

    // -----------------------------------------
    // Run visual odometry
    // -----------------------------------------
    std::vector<FeaturePoint> oldFeaturePointsLeft;
    std::vector<FeaturePoint> currentFeaturePointsLeft;

    for (int frame_id = init_frame_id+1; frame_id < 9000; frame_id++)
    {

        std::cout << std::endl << "frame id " << frame_id << std::endl;
        // ------------
        // Load images
        // ------------
        cv::Mat imageRight_t1,  imageLeft_t1;
        if(use_intel_rgbd)
        {
            pCamera->getLRFrames(imageLeft_t1,imageRight_t1);
        }
        else
        {
            cv::Mat imageLeft_t1_color;
            loadImageLeft(imageLeft_t1_color,  imageLeft_t1, frame_id, filepath);        
            cv::Mat imageRight_t1_color;  
            loadImageRight(imageRight_t1_color, imageRight_t1, frame_id, filepath);            
        }


        t_a = clock();
        std::vector<cv::Point2f> oldPointsLeft_t0 = currentVOFeatures.points;


        std::vector<cv::Point2f> pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1;  
        matchingFeatures( imageLeft_t0, imageRight_t0,
                          imageLeft_t1, imageRight_t1, 
                          currentVOFeatures,
                          pointsLeft_t0, 
                          pointsRight_t0, 
                          pointsLeft_t1, 
                          pointsRight_t1);  

        imageLeft_t0 = imageLeft_t1;
        imageRight_t0 = imageRight_t1;

        std::vector<cv::Point2f>& currentPointsLeft_t0 = pointsLeft_t0;
        std::vector<cv::Point2f>& currentPointsLeft_t1 = pointsLeft_t1;
        
        std::vector<cv::Point2f> newPoints;
        std::vector<bool> valid; // valid new points are ture

        // ---------------------
        // Triangulate 3D Points
        // ---------------------
        cv::Mat points3D_t0, points4D_t0;
        cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);
        cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);

        // ---------------------
        // Tracking transfomation
        // ---------------------
	clock_t tic_gpu = clock();
        trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t0, pointsLeft_t1, points3D_t0, rotation, translation, false);
	clock_t toc_gpu = clock();
	    std::cerr << "tracking frame 2 frame: " << float(toc_gpu - tic_gpu)/CLOCKS_PER_SEC*1000 << "ms" << std::endl;

        displayTracking(imageLeft_t1, pointsLeft_t0, pointsLeft_t1);

        cv::Rodrigues(rotation, rvec);
        camPose = { rvec.at<double>(0,0), rvec.at<double>(1,0),rvec.at<double>(2,0), translation.at<double>(0,0), translation.at<double>(1,0), translation.at<double>(2,0) }; 

        // ------------------------------------------------
        // Intergrating and display
        // ------------------------------------------------
        cv::Vec3f rotation_euler = rotationMatrixToEulerAngles(rotation);

        cv::Mat rigid_body_transformation;

        // std::cout<< " points3D_t0 : " << points3D_t0.size() << "  pointsLeft_t1: " << pointsLeft_t1.size() << std::endl;

        //cout << "old rotation and trans : "<< rvec.at<double>(0,0) << " - "<< rvec.at<double>(1,0) << " - " << rvec.at<double>(2,0)<< " - "
        //        << translation.at<double>(0,0) << "- "<<  translation.at<double>(1,0) << " - " << translation.at<double>(2,0)<< std::endl;

        if(abs(rotation_euler[1])<0.1 && abs(rotation_euler[0])<0.1 && abs(rotation_euler[2])<0.1)
        {
            poses.pop_front();
            poses.push_back(camPose);

            points3D.pop_front();  
            points3D.push_back(points3D_t0);

            points2D.pop_front();
            points2D.push_back(pointsLeft_t1);
            
            if(frame_id > windowSize)
            {
                optimize(points3D, points2D, poses, intrinsic);
                const auto& currentPose = poses.back() ; 


                rvec1.at<double>(0,0) = currentPose[0];
                rvec1.at<double>(1,0) = currentPose[1];
                rvec1.at<double>(2,0) = currentPose[2];
                
                translation.at<double>(0,0) = currentPose[3];
                translation.at<double>(1,0) = currentPose[4];
                translation.at<double>(2,0) = currentPose[5];    

                cv::Rodrigues(rvec1,rotation);
            }

            //cout << "updated rotation and trans : " << rvec1 << " ____ " << translation << std::endl; 

            integrateOdometryStereo(frame_id, rigid_body_transformation, frame_pose, rotation, translation, scale);
        }
        else 
            std::cout << "Too large rotation"  << std::endl;

        t_b = clock();
        float frame_time = 1000*(double)(t_b-t_a)/CLOCKS_PER_SEC;
        float fps = 1000/frame_time;
        cout << "[Info] frame times (ms): " << frame_time << endl;
        cout << "[Info] FPS: " << fps << endl;

        // std::cout << "rigid_body_transformation" << rigid_body_transformation << std::endl;
        // std::cout << "rotation: " << rotation_euler << std::endl;
        // std::cout << "translation: " << translation.t() << std::endl;
        // std::cout << "frame_pose" << frame_pose << std::endl;

        cv::Mat xyz = frame_pose.col(3).clone();
        display(frame_id, trajectory, xyz, pose_matrix_gt, fps, display_ground_truth);

    }

    return 0;
}

